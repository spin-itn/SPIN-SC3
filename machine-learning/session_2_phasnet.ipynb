{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "241827c2",
      "metadata": {
        "id": "241827c2"
      },
      "source": [
        "# 2. Understanding and training PhaseNet on a local dataset\n",
        "\n",
        "Re-made from the [PhaseNet tutorial](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03a_training_phasenet.ipynb) provided by the [SeisBench](https://seisbench.readthedocs.io/en/latest/) project  by [Léonard Seydoux](https://sites.google.com/view/leonard-seydoux/accueil) for the [Short Course #3](https://spin-itn.eu/sc3/) of the [Innovative Training Network SPIN](https://spin-itn.eu/) (Seismological Parameters and INstrumentation).\n",
        "\n",
        "> This notebook is largely inspired by the [PhaseNet tutorial](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03a_training_phasenet.ipynb) provided by the [SeisBench](https://seisbench.readthedocs.io/en/latest/) project. You can find other interesting tutorials therein. This tutorial shows how to use and re-train the [PhaseNet](https://github.com/AI4EPS/PhaseNet) model with SeisBench. Note that you can also use the distribution of PhaseNet provided in the [dedicated repository](https://github.com/AI4EPS/PhaseNet) and use another version of PyTorch adapted to your hardware. You can find a list of compatible versions of PyTorch at https://pytorch.org/get-started/locally.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/seisbench/seisbench/main/docs/_static/seisbench_logo_subtitle_outlined.svg\" width=300px alt=\"SeisBench logo\"/>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "98a7259a",
      "metadata": {},
      "source": [
        "## 1. Introduction\n",
        "\n",
        "The tutorial here works with ~5BG of continuous seismic data from Iquique, in the sequence of aftershocks that followed the $M_w$ 8.1 Iquique earthquake that occurred in northern Chile in 2014.\n",
        "\n",
        "The data is stored in a SeisBench dataset, and should be downloaded beforehand. If you have little space left on your computer or have access to other datasets, it is also possible to use it. Note that re-training PhaseNet requires labels (e.g., _P_ and _S_ wave picks) for the training data. If you have access to such data, you can use it to re-train PhaseNet. If not, you can still use the pre-trained model to make predictions on your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "689c1ea9",
      "metadata": {
        "id": "689c1ea9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from seisbench.util import worker_seeding\n",
        "from seisbench.models import PhaseNet\n",
        "from seisbench.data import WaveformDataset\n",
        "from seisbench.generate import GenericGenerator\n",
        "import seisbench.generate as sbg\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15091ad7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is just to render plots in vector format\n",
        "%config InlineBackend.figure_formats = \"svg\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6b9649cd",
      "metadata": {
        "id": "6b9649cd"
      },
      "source": [
        "## 2. Data preparation\n",
        "\n",
        "First we will get the dataset ready with the SeisBench API. We will use the `seisbench.data` module to load the data. We will do all examples with the Iquique dataset that contains 13,400 examples of picked arrivals from the aftershock sequence following the $M_w$ 8.1 Iquique earthquake that occurred in northern Chile in 2014. All stations are 100 Hz, 3-component stations. The waveforms contain examples of earthquakes only, and was published in Woollam et al. ([2019](https://doi.org/10.1785/0220180312)). \n",
        "\n",
        "### 2.1. Downloading or copying the dataset\n",
        "\n",
        "The dataset is available from the SeisBench website, and can be downloaded with instanciating one of the dataset's name (see https://seisbench.readthedocs.io/en/stable/pages/benchmark_datasets.html for an overview of available datasets in SeisBench. Yet, downloading a dataste might take a while and we recommend that you pick up the data from the available USB sticks. You can choose a location to copy the dataset to, and load it with the following command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71e9514",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset from disk\n",
        "data = WaveformDataset(\"../iquique\", name=\"Iquique\")\n",
        "\n",
        "# Show loaded dataset\n",
        "print(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ebdd989a",
      "metadata": {},
      "source": [
        "### 2.2 Dataset's content\n",
        "\n",
        "The datasets consists essentially in two things that you can directly see in the `dataset` repository:\n",
        "\n",
        "- The __metadata__, stored as a csv file, and loaded into a pandas dataframe. This dataframe contains the information about the waveforms and events, such as the station, the event time, the picked phases, sampling rates, etc. If can be accessed with the `metadata` attribute of the `dataset`.\n",
        "\n",
        "- The __waveforms__, stored as entries of an hdf5 file. They are loaded dynamically in the memory, so you don't have to load all waveforms at once. Several methods are available to access the waveforms, depeding on if you want to train a model, make predictions or just have a look at the data.\n",
        "\n",
        "#### Having a look at metadata\n",
        "\n",
        "You can access them with the `metadata` attribute of the `dataset`. For example, you can have a look at the first 5 rows of the metadata with the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff0bc993",
      "metadata": {},
      "outputs": [],
      "source": [
        "data.metadata.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e6fbdb30",
      "metadata": {},
      "source": [
        "#### Having a look at waveforms\n",
        "\n",
        "We can also have a look at a random sample in the dataset, and the corresponding waveform. The `get_waveforms` methods only  returns the waveforms as a `np.numpy` array, while the `get_sample` method returns both the waveforms and the metadata, which we can use to annotate the waveforms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e3ac053",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a sample\n",
        "waveforms, info = data.get_sample(0)\n",
        "\n",
        "# Plot and annotate\n",
        "utils.plot_waveforms(waveforms, info)\n",
        "utils.add_picks(info)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3da8a152",
      "metadata": {},
      "source": [
        "### 2.3 Data generator\n",
        "\n",
        "Pytorch implements a data generator that allows to load data in batches easily. This is useful for training and evaluating the model. The `seisbench.generator` module provides a `GenericGenerator` class that can be used to generate batches of waveforms. The generator can be instanciated with a `dataset` object. \n",
        "\n",
        "With the generator we can add augmentations, which are here used to segment the input data into adequate segments. Note that the model only works with 3001-points long waveforms, so we need to segment the waveforms. We can do this with the `add_augmentation` module, which provides a `Segment` augmentation that can be used to segment the waveforms. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c7eeb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a data generator\n",
        "data_generator = sbg.GenericGenerator(data)\n",
        "\n",
        "# Normalize and label\n",
        "augmentations = [\n",
        "    sbg.WindowAroundSample(list(utils.PHASES.keys()), samples_before=3000, windowlen=6000, selection=\"random\", strategy=\"variable\"),\n",
        "    sbg.RandomWindow(windowlen=3001, strategy=\"pad\"),\n",
        "    sbg.Normalize(demean_axis=-1, amp_norm_axis=-1, amp_norm_type=\"peak\"),\n",
        "    sbg.ChangeDtype(np.float32),\n",
        "    sbg.ProbabilisticLabeller(label_columns=utils.PHASES, sigma=30, dim=0)\n",
        "]\n",
        "\n",
        "data_generator.add_augmentations(augmentations)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f6f813a2",
      "metadata": {},
      "source": [
        "## 3. Loading a model\n",
        "\n",
        "First, we can investigate pre-trained PhaseNet models. We can load it using `seisbench.models`. Firstly, we can access the list of available pre-trained models on different datasets, also available from SeisBench. The list returns the name of the datasets that PhaseNet was trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b596d33f",
      "metadata": {
        "id": "b596d33f"
      },
      "outputs": [],
      "source": [
        "# List available PhaseNet models\n",
        "print(f\"Pretrained models for PhaseNet: {PhaseNet.list_pretrained()}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fcba7b21",
      "metadata": {},
      "source": [
        "### 3.1 Pre-trained model loading\n",
        "\n",
        "Then, we can download one of the model to use for our experiment. A model is really light (several MB), and can be downloaded in a few seconds. We can load it with the `from_pretrained` method. The function returns a `seisbench.models.Model` object, which is a wrapper around the model. The model is a PyTorch model, and can be used as such. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67b4ecca",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a pretrained model\n",
        "model = PhaseNet.from_pretrained(\"iquique\")\n",
        "\n",
        "print(dir(model))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2f0d7132",
      "metadata": {},
      "source": [
        "### 3.2 Observe a forward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "941803ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a random sample\n",
        "waveform_index = np.random.randint(0, len(data_generator))\n",
        "sample = data_generator[waveform_index]\n",
        "x = sample[\"X\"]\n",
        "y_true = sample[\"y\"]\n",
        "metadata = data.metadata.iloc[waveform_index]\n",
        "\n",
        "# Forward pass (no need to calculate gradient since we just want to predict)\n",
        "with torch.no_grad():\n",
        "    sample_tensor = torch.tensor(x, device=model.device).unsqueeze(0)\n",
        "    y_hat = model.forward(sample_tensor)\n",
        "    y_hat = y_hat.cpu().numpy().squeeze()\n",
        "\n",
        "# Plot y_hat\n",
        "ax = utils.plot_waveforms_and_labels(x, y_true, y_hat, metadata=metadata)\n",
        "ax[-1].set_ylabel(\"Prediction\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9f4ac68f",
      "metadata": {},
      "source": [
        "### 3.3 Errors\n",
        "\n",
        "We here redefined the loss that was used in the paper. This loss is the cross-entropy loss, as defined by the following equation:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = - \\sum_c \\sum_x p_{c}(x) \\log \\hat{p}_{c}(x)\n",
        "$$\n",
        "\n",
        "where $p_{c}(x)$ is the probability of the $c$-th class at the $x$-th point, and $\\hat{p}_{c}(x)$ is the predicted probability of the $c$-th class at the $x$-th point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27bcafe0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(y_pred, y_true, eps=1e-5):\n",
        "        # vector cross entropy loss\n",
        "        h = y_true * torch.log(y_pred + eps)\n",
        "        h = h.mean(-1).sum(-1)  # Mean along sample dimension and sum along pick dimension\n",
        "        h = h.mean()  # Mean over batch axis\n",
        "        return -h.cpu().numpy()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e2e4d794",
      "metadata": {},
      "source": [
        "We can evaluate the performances of our model over a set of samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f975896",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect \n",
        "losses = []\n",
        "for waveform_index in range(500):\n",
        "\n",
        "    sample = data_generator[waveform_index]\n",
        "    x = sample[\"X\"]\n",
        "    y_true = sample[\"y\"]\n",
        "    metadata = data.metadata.iloc[waveform_index]\n",
        "\n",
        "    # Forward\n",
        "    with torch.no_grad():\n",
        "        sample_tensor = torch.tensor(x, device=model.device).unsqueeze(0)\n",
        "        y_hat = model.forward(sample_tensor)\n",
        "        y_hat = y_hat.cpu().numpy().squeeze()\n",
        "\n",
        "    # Plot y_hat\n",
        "    losses.append(loss_fn(torch.tensor(y_hat), torch.tensor(y_true)))\n",
        "\n",
        "# Plot losses of a fiew samples\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(losses, bins=30)\n",
        "ax.set_xlabel(\"Loss (cross entropy)\")\n",
        "ax.set_ylabel(\"Count\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "50a93b04",
      "metadata": {},
      "source": [
        "## 4. Retrain the model\n",
        "\n",
        "First, we need to define a training and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "015e91af",
      "metadata": {},
      "outputs": [],
      "source": [
        "train, dev, test = data.train_dev_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc7cfc9",
      "metadata": {
        "id": "8bc7cfc9"
      },
      "source": [
        "Now we define two generators with identical augmentations, one for training, one for validation. The augmentations are:\n",
        "1. Selection of a (long) window around a pick. This way, we ensure that out data always contains a pick.\n",
        "1. Selection of a random window with 3001 samples, the input length of PhaseNet.\n",
        "1. A normalization, consisting of demeaning and amplitude normalization.\n",
        "1. A change of datatype to float32, as this is expected by the pytorch model.\n",
        "1. A probabilistic label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cce52377",
      "metadata": {
        "id": "cce52377"
      },
      "outputs": [],
      "source": [
        "train_generator = sbg.GenericGenerator(train)\n",
        "dev_generator = sbg.GenericGenerator(dev)\n",
        "\n",
        "augmentations = [\n",
        "    sbg.WindowAroundSample(list(utils.PHASES.keys()), samples_before=3000, windowlen=6000, selection=\"random\", strategy=\"variable\"),\n",
        "    sbg.RandomWindow(windowlen=3001, strategy=\"pad\"),\n",
        "    sbg.Normalize(demean_axis=-1, amp_norm_axis=-1, amp_norm_type=\"peak\"),\n",
        "    sbg.ChangeDtype(np.float32),\n",
        "    sbg.ProbabilisticLabeller(label_columns=utils.PHASES, sigma=30, dim=0)\n",
        "]\n",
        "\n",
        "train_generator.add_augmentations(augmentations)\n",
        "dev_generator.add_augmentations(augmentations)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "454e75a3",
      "metadata": {
        "id": "454e75a3"
      },
      "source": [
        "Let's visualize a few training examples. Everytime you run the cell below, you'll see a different training example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7ef0570",
      "metadata": {
        "id": "d7ef0570",
        "outputId": "f2b2220a-551e-4c32-8fee-f37138108c2d"
      },
      "outputs": [],
      "source": [
        "sample = train_generator[np.random.randint(len(train_generator))]\n",
        "metadata = train.metadata.iloc[waveform_index]\n",
        "x = sample[\"X\"]\n",
        "y_true = sample[\"y\"]\n",
        "\n",
        "# Plot y_hat\n",
        "ax = utils.plot_waveforms_and_labels(x, y_true, y_true * np.nan, metadata=metadata)\n",
        "ax[-1].set_ylabel(\"Prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "005455e0",
      "metadata": {
        "id": "005455e0"
      },
      "source": [
        "SeisBench generators are pytorch datasets. Therefore, we can pass them to pytorch data loaders. These will automatically take care of parallel loading and batching. Here we create one loader for training and one for validation. We choose a batch size of 256 samples. This batch size should fit on most hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf2e42c",
      "metadata": {
        "id": "2bf2e42c"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "num_workers = 4  # The number of threads used for loading data\n",
        "\n",
        "train_loader = DataLoader(train_generator, batch_size=batch_size, shuffle=True, num_workers=num_workers, worker_init_fn=worker_seeding)\n",
        "dev_loader = DataLoader(dev_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0e9b009a",
      "metadata": {
        "id": "0e9b009a"
      },
      "source": [
        "### 2.1 Training and testing ingredients"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9024bf3c",
      "metadata": {
        "id": "9024bf3c"
      },
      "source": [
        "Now we got all components for training the model. What we still need to do is define the optimizer and the loss, and write the training and validation loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef5581f",
      "metadata": {
        "id": "5ef5581f"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-2\n",
        "epochs = 5\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b9e5e96",
      "metadata": {
        "id": "4b9e5e96"
      },
      "outputs": [],
      "source": [
        "def loss_fn(y_pred, y_true, eps=1e-5):\n",
        "        # vector cross entropy loss\n",
        "        h = y_true * torch.log(y_pred + eps)\n",
        "        h = h.mean(-1).sum(-1)  # Mean along sample dimension and sum along pick dimension\n",
        "        h = h.mean()  # Mean over batch axis\n",
        "        return -h\n",
        "\n",
        "def train_loop(dataloader):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch_id, batch in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(batch[\"X\"].to(model.device))\n",
        "        loss = loss_fn(pred, batch[\"y\"].to(model.device))\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_id % 5 == 0:\n",
        "            loss, current = loss.item(), batch_id * batch[\"X\"].shape[0]\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader):\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            pred = model(batch[\"X\"].to(model.device))\n",
        "            test_loss += loss_fn(pred, batch[\"y\"].to(model.device)).item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    print(f\"Test avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f252b88f",
      "metadata": {},
      "source": [
        "### 2.2 Training loop\n",
        "\n",
        "This loop performs the optimization by training (forward pass, backpropagation, weight update) on the training data. We use the Adam optimizer, which is a good default choice. We use the cross-entropy loss, which is the loss used in the paper.\n",
        "\n",
        "### 2.3 Test loop   \n",
        "\n",
        "This loop evaluates the model on the validation data. We use the cross-entropy loss, which is the loss used in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "975a1cd8",
      "metadata": {
        "id": "975a1cd8",
        "outputId": "4c2f9c1f-e96f-4492-b1eb-7bd26de267cd"
      },
      "outputs": [],
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_loader)\n",
        "    test_loop(dev_loader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "93a43e14",
      "metadata": {
        "id": "93a43e14"
      },
      "source": [
        "### 2.3 Evaluating the model\n",
        "\n",
        "Not that we trained the model, we can evaluate it. First, we'll check how the model does on an example from the development set. Note that the model will most likely not be fully trained after only five epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c6a38a2",
      "metadata": {
        "id": "7c6a38a2",
        "outputId": "cfc6dde0-1f97-4245-cdf6-37a0dad14a06"
      },
      "outputs": [],
      "source": [
        "sample = dev_generator[np.random.randint(len(dev_generator))]\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred = model(torch.tensor(sample[\"X\"], device=model.device).unsqueeze(0))  # Add a fake batch dimension\n",
        "    y_pred = y_pred[0].cpu().numpy()\n",
        "\n",
        "sample = train_generator[np.random.randint(len(train_generator))]\n",
        "metadata = train.metadata.iloc[waveform_index]\n",
        "x = sample[\"X\"]\n",
        "y_true = sample[\"y\"]\n",
        "\n",
        "# Plot y_hat\n",
        "ax = utils.plot_waveforms_and_labels(x, y_true, y_pred, metadata=metadata)\n",
        "ax[-1].set_ylabel(\"Prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d48017ff",
      "metadata": {
        "id": "d48017ff"
      },
      "source": [
        "As a second option, we'll directly apply our model to an obspy waveform stream using the `annotate` function. For this, we are downloading waveforms through FDSN and annotating them. Note that you could use the `classify` function in a similar fashion.\n",
        "\n",
        "As we trained the model on Swiss data, we use an example event from Switzerland. Note that we deliberately chose a rather easy example, as the model is not fully trained after the low number of epochs. The exact performance of the model will vary depending, because the model training and initialization involves random aspects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067fcf5d",
      "metadata": {
        "id": "067fcf5d",
        "outputId": "894bbb65-c11d-4ccd-c85f-905225dd967e"
      },
      "outputs": [],
      "source": [
        "from obspy.clients.fdsn import Client\n",
        "from obspy import UTCDateTime\n",
        "\n",
        "client = Client(\"ETH\")\n",
        "\n",
        "t = UTCDateTime(\"2019-11-04T00:59:46.419800Z\")\n",
        "stream = client.get_waveforms(network=\"CH\", station=\"EMING\", location=\"*\", channel=\"HH?\", starttime=t-30, endtime=t+50)\n",
        "\n",
        "annotations = model.annotate(stream)\n",
        "\n",
        "print(annotations)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.subplots(2, 1, sharex=True)\n",
        "\n",
        "offset = annotations[0].stats.starttime - stream[0].stats.starttime\n",
        "for i, (trace, annotation) in enumerate(zip(stream, annotations)):\n",
        "    data = trace.data / np.max(np.abs(trace.data))\n",
        "    ax[0].plot(trace.times(), data + i, label=trace.stats.channel)\n",
        "    if annotation.stats.channel[-1] != \"N\":\n",
        "        ax[1].plot(annotation.times() + offset, annotation.data, label=annotation.stats.channel)\n",
        "\n",
        "ax[0].set_xlabel(\"Time (s)\")\n",
        "ax[1].set_xlabel(\"Time (s)\")\n",
        "ax[1].set_ylabel(\"Probability\")\n",
        "ax[0].legend()\n",
        "ax[1].legend()\n",
        "ax[0].grid()\n",
        "ax[1].grid()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
